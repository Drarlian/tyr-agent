import time
import json
import google.generativeai as genai
from typing import List, Optional, Callable
from functions.ai_functions.history_storage import HistoryStorage
from datetime import datetime
from functions.ai_functions.gemini_config import configure_gemini  # Configurando a conex√£o com Gemini.

# genai.configure(api_key=GEMINI_KEY)
# basic_model = genai.GenerativeModel('gemini-2.5-flash-preview-04-17')
# real_time_model = genai.GenerativeModel("gemini-1.5-flash")  # -> Modelo com suporte a streaming.


class SimpleAgent:
    MAX_ALLOWED_HISTORY = 20

    def __init__(self, prompt_build: str, agent_name: str, model: genai.GenerativeModel, storage: Optional[HistoryStorage] = None, max_history: int = 20):
        self.prompt_build: str = prompt_build
        self.agent_name: str = agent_name
        self.storage: HistoryStorage = storage or HistoryStorage(f"{agent_name.lower()}_history.json")
        self.historic: List[dict] = self.storage.load_history(agent_name)

        self.agent_model: genai.GenerativeModel = model

        self.MAX_HISTORY = min(max_history, self.MAX_ALLOWED_HISTORY)
        self.PROMPT_TEMPLATE = """
        {role}

        Hist√≥rico de Conversas:
        {history}

        Mensagem atual:
        {current}
        """

    def chat(self, user_input: str, streaming: bool = False) -> str | None:
        prompt = self.generate_prompt(user_input)

        try:
            if streaming:
                print("üß† Gemini est√° digitando:\n")
                response = self.agent_model.generate_content(prompt, stream=True)

                final_text: str = ""
                for chunk in response:
                    print(chunk.text, end='', flush=True)
                    final_text += chunk.text
                    time.sleep(0.04)  # -> "Efeito" de digitando.

                print("\n\n‚úÖ Fim da resposta.")

                self.update_historic(user_input, final_text)
                return final_text
            else:
                response = self.agent_model.generate_content(prompt, stream=True)
                response.resolve()
                self.update_historic(user_input, response.text)
                return response.text

        except Exception as e:
            print(f"‚ùå Erro: {e}")
            return None

    def update_historic(self, user_input: str, agent_response: str):
        actual_conversation = {
            "Data": datetime.now().strftime("%d/%m/%Y %H:%M:%S"),
            "Mensagem": {
                "Usuario": user_input,
                self.agent_name: agent_response,
            }
        }

        self.historic.append(actual_conversation)
        self.historic = self.historic[-self.MAX_HISTORY:]  # -> Mantendo apenas os N itens no hist√≥rico.
        self.storage.save_history(self.agent_name, actual_conversation)

    def generate_prompt(self, promp_text: str) -> str:
        formatted_history = "\n".join(
            f"{item['Data']} - Usu√°rio: {item['Mensagem']['Usuario']}\n{self.agent_name}: {item['Mensagem'][self.agent_name]}"
            for item in self.historic
        )

        return self.PROMPT_TEMPLATE.format(
            role=self.prompt_build,
            history=formatted_history if self.historic else 'N√£o consta.',
            current=promp_text
        )


class ComplexAgent(SimpleAgent):
    MAX_ALLOWED_HISTORY = 20

    def __init__(self, prompt_build: str, agent_name: str, model: genai.GenerativeModel, functions: Optional[dict[str, Callable]], storage: Optional[HistoryStorage] = None, max_history: int = 20):
        super().__init__(prompt_build, agent_name, model, storage, max_history)
        self.functions: dict[str, Callable] = functions or {}

        self.PROMPT_TEMPLATE = """
        {role}

        Voc√™ tem acesso √†s seguintes fun√ß√µes que podem ser utilizadas para responder perguntas do usu√°rio:
        {function_list}

        Sempre que identificar que precisa executar uma ou mais fun√ß√µes para responder corretamente, gere uma resposta no formato JSON no seguinte formato:
        {call_function_explanation}

        Hist√≥rico de Conversas:
        {history}

        Mensagem atual:
        {current}
        """

    def chat_with_functions(self, user_input: str, streaming: bool = False) -> str | None:
        # Primeira rodada: input original
        prompt = self.__generate_prompt_with_functions(user_input)
        response = self.agent_model.generate_content(prompt, stream=True)
        response.resolve()
        response_text = response.text.strip()

        func_calls = self.__extract_function_calls(response_text)

        if not func_calls:
            self.update_historic(user_input, response_text)
            return response_text

        # Executa m√∫ltiplas fun√ß√µes solicitadas
        results = {}
        for call in func_calls:
            result = self.__execute_function(call)
            results[call['function_name']] = result

        # Segunda rodada: prompt enriquecido com resultados
        enriched_prompt = f"""
        {self.prompt_build}

        O agente solicitou a execu√ß√£o das seguintes fun√ß√µes:
        {json.dumps(results, indent=2)}

        Mensagem original do usu√°rio:
        {user_input}

        Agora gere uma resposta final ao usu√°rio com base nos resultados das fun√ß√µes.
        """

        final_response = self.agent_model.generate_content(enriched_prompt, stream=streaming)

        if streaming:
            final_text = ""
            for chunk in final_response:
                print(chunk.text, end="", flush=True)
                final_text += chunk.text
                time.sleep(0.04)
            self.update_historic(user_input, final_text)
            return final_text.strip()

        final_response.resolve()
        final_text = final_response.text.strip()
        self.update_historic(user_input, final_text)
        return final_text

    def __extract_function_calls(self, response_text: str) -> list[dict]:
        try:
            response_text = response_text.removeprefix('```json\n').removesuffix("\n```")
            response_text = response_text.replace("\n", "").replace("`", "").replace("¬¥", "")
            data = json.loads(response_text)
            if isinstance(data, list):
                return data
            elif isinstance(data, dict):
                return [data] if data.get("call_function") else []
            return []
        except json.JSONDecodeError:
            return []

    def __execute_function(self, call: dict) -> str:
        name = call.get("function_name")
        params = call.get("parameters", {})
        func = self.functions.get(name)

        if not func:
            return f"‚ùå Fun√ß√£o '{name}' n√£o encontrada."

        try:
            result = func(**params)
            return f"‚úÖ Resultado da fun√ß√£o '{name}': {result}"
        except Exception as e:
            return f"‚ùå Erro ao executar '{name}': {e}"

    def __generate_prompt_with_functions(self, promp_text: str) -> str:
        formatted_history = "\n".join(
            f"{item['Data']} - Usu√°rio: {item["Mensagem"]['Usuario']}\n{self.agent_name}: {item["Mensagem"][self.agent_name]}"
            for item in self.historic
        )

        function_list = "\n".join(
            f"- {name}({', '.join(f.__code__.co_varnames[:f.__code__.co_argcount])})" for name, f in
            self.functions.items())

        call_function_explanation = '{"call_function": true, "function_name": "nome_da_funcao", "parameters": {"parametro_1": valor_parametro_1, "parametro_n": valor_parametro_n}, "mensagem_ao_usuario": "texto explicativo amig√°vel"}'

        return self.PROMPT_TEMPLATE.format(
            role=self.prompt_build,
            function_list=function_list,
            call_function_explanation= call_function_explanation,
            history=formatted_history if self.historic else 'N√£o consta.',
            current=promp_text
        )


def check_models():
    models = genai.list_models()
    for model in models:
        print(model)


# Exemplo de uso das fun√ß√µes
def somar(a: float, b: float) -> float:
    return a + b


def pegar_clima(cidade: str) -> str:
    return f"Clima em {cidade}: Ensolarado 28¬∞C"


if __name__ == '__main__':
    configure_gemini()
    model_test = genai.GenerativeModel('gemini-2.5-flash-preview-04-17')
    # weather_agent = SimpleAgent("Voc√™ √© um agente respons√°vel por fornecer apenas informa√ß√µes sobre o clima.", "WeatherAgent", model_test)
    # teste = weather_agent.chat("Me fale sobre o clima de Miami atualmente.", True)
    # print('-' * 30)
    # print(teste)
    # print(weather_agent.historic)

    functions_test = {
        "pegar_clima": pegar_clima,
        "somar": somar
    }
    test_complext = ComplexAgent("Voc√™ √© um agente respons√°vel por fornecer apenas informa√ß√µes sobre o clima e sobre soma de numeros.", "WeatherSumAgent", model_test, functions_test)
    test_complext.chat_with_functions("Me fale sobre o clima de Belo Horizonte atualmente. Tamb√©m me diga quanto √© 73+15", True)
    print()
    print('-' * 30)
    print(test_complext.historic)
