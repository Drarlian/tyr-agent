import os
import time
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()

GEMINI_KEY = os.getenv('GEMINI_KEY')

genai.configure(api_key=GEMINI_KEY)
basic_model = genai.GenerativeModel('gemini-2.5-flash-preview-04-17')
real_time_model = genai.GenerativeModel("gemini-1.5-flash")  # -> Modelo com suporte a streaming.


def generate_prompt(promp_text: str, historic: str = "N√£o consta.") -> str:
    return f"""
    Voc√™ √© um agente respons√°vel por auxiliar o usu√°rio em suas duvidas sobre o clima, informa√ß√µes referente ao hist√≥rico dele sobre perguntas do clima e nada mais.
    Interaja com o usu√°rio baseado no hist√≥rico de mensagens abaixo.
    Responda a mensagem atual do usu√°rio.
    {historic}

    Mensagem atual:
    {promp_text}
    """


def basic_conversation(promp_text: str, historic: str) -> str:
    prompt = generate_prompt(promp_text, historic)

    # response = basic_model.generate_content(prompt)
    # return response.text

    response = basic_model.generate_content(prompt, stream=True)
    response.resolve()
    # Quando usamos o stream=True, a resposta (response) √© um iterador de chunks, cada um com um peda√ßo do texto.
    # O .resolve() percorre todos esses chunks internamente, concatena os textos e gera o conte√∫do final completo.
    # Usar stream=True com .resolve() √© naturalmente mais r√°pido para se obter a resposta que o stream=False.
    # √â mais r√°pido em todos os cen√°rios.
    # ‚Ä¢ Tanto para conversa em tempo real (s√≥ √© uma op√ß√£o com o stream=True)
    # ‚Ä¢ Quanto para apenas entregar a resposta final.

    print('-' * 30)
    print(response)
    print('-' * 30)

    return response.text


def real_time_conversation(prompt: str):
    print("üß† Gemini est√° digitando:\n")

    try:
        response = real_time_model.generate_content(generate_prompt(prompt), stream=True)

        for chunk in response:
            print(chunk.text, end='', flush=True)
            time.sleep(0.03)  # S√≥ pra dar um efeito "digitando".

        print("\n\n‚úÖ Fim da resposta.")

    except Exception as e:
        print(f"\n‚ùå Erro: {e}")


def check_models():
    models = genai.list_models()
    for model in models:
        print(model)


if __name__ == '__main__':
    test_historic = """
    Mensagem Usu√°rio: Ol√° eu me chamo Teste! 

    Resposta do agente anterior: Ol√°, Teste!
    Estou funcionando perfeitamente, obrigado por perguntar! üòä
    Prazer em te conhecer! Como posso te ajudar hoje?"))

    Mensagem Usu√°rio: Queria saber como esta o clima em Paris

    Resposta do agente anterior: 
    Certo, Teste! Posso te informar sobre o clima em Paris agora.

    Em Paris, a temperatura √© de aproximadamente 15¬∞C, com c√©u parcialmente nublado. A sensa√ß√£o t√©rmica √© similar. A umidade est√° em torno de 70% e o vento sopra fraco.
    """
    print(basic_conversation("Me diga como esta o clima nos lugares que eu costumo perguntar", test_historic))
